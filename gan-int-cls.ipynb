{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5140550,"sourceType":"datasetVersion","datasetId":2534241}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T02:18:07.372663Z","iopub.execute_input":"2025-05-05T02:18:07.372858Z","iopub.status.idle":"2025-05-05T02:20:07.606157Z","shell.execute_reply.started":"2025-05-05T02:18:07.372830Z","shell.execute_reply":"2025-05-05T02:20:07.605336Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -qU torchtext spacy\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T02:20:07.607087Z","iopub.execute_input":"2025-05-05T02:20:07.607619Z","iopub.status.idle":"2025-05-05T02:20:37.425403Z","shell.execute_reply.started":"2025-05-05T02:20:07.607574Z","shell.execute_reply":"2025-05-05T02:20:37.424398Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.2/29.2 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nen-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: en-core-web-sm\n  Attempting uninstall: en-core-web-sm\n    Found existing installation: en-core-web-sm 3.7.1\n    Uninstalling en-core-web-sm-3.7.1:\n      Successfully uninstalled en-core-web-sm-3.7.1\nSuccessfully installed en-core-web-sm-3.8.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport spacy\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T02:20:37.426470Z","iopub.execute_input":"2025-05-05T02:20:37.426710Z","iopub.status.idle":"2025-05-05T02:20:43.838998Z","shell.execute_reply.started":"2025-05-05T02:20:37.426689Z","shell.execute_reply":"2025-05-05T02:20:43.838373Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n# Dataset configuration\nclass CUBDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_dir = os.path.join(root_dir, 'CUB_200_2011/images')\n        self.text_path = os.path.join(root_dir, 'cvpr2016_cub/text_c10')\n        \n        # Load image metadata\n        self.image_df = pd.read_csv(os.path.join(root_dir, 'CUB_200_2011/images.txt'), \n                                  sep=' ', names=['img_id', 'img_path'])\n        self.split_df = pd.read_csv(os.path.join(root_dir, 'CUB_200_2011/train_test_split.txt'),\n                                  sep=' ', names=['img_id', 'is_training'])\n        \n        # Filter training images\n        self.train_images = self.image_df.merge(self.split_df, on='img_id').query('is_training == 1')\n        \n        # Text processing\n        self.nlp = spacy.load('en_core_web_sm')\n\n    def __len__(self):\n        return len(self.train_images)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.train_images.iloc[idx]['img_path'])\n        image = Image.open(img_path).convert('RGB')\n        \n        # Text processing (using first caption for simplicity)\n        text_file = os.path.join(self.text_path, \n                               self.train_images.iloc[idx]['img_path'].replace('.jpg', '.txt'))\n        with open(text_file, 'r') as f:\n            caption = f.readline().strip()\n        \n        doc = self.nlp(caption)\n        tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        # In CUBDataset __getitem__:\n        return image, caption  # Return raw caption string instead of processed tokens\n\n\n# Image transformations\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Initialize dataset\ndataset = CUBDataset(root_dir='/kaggle/input/cub2002011',\n                    transform=transform)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n\n# %% [code]\n# Model Architecture (GAN-INT-CLS)\nclass Generator(nn.Module):\n    def __init__(self, text_embed_dim=128, noise_dim=100):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(noise_dim + text_embed_dim, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, noise, text_embed):\n        text_embed = text_embed.view(-1, 128, 1, 1)\n        combined = torch.cat([noise.view(-1, 100, 1, 1), text_embed], 1)\n        return self.main(combined)\n\nclass Discriminator(nn.Module):\n    def __init__(self, text_embed_dim=128):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        \n        self.text_embed_proj = nn.Linear(text_embed_dim, 512)\n        self.discriminator = nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n\n    def forward(self, image, text_embed):\n        features = self.main(image)\n        text_proj = self.text_embed_proj(text_embed).view(-1, 512, 1, 1)\n        combined = features * text_proj\n        return self.discriminator(combined).view(-1)\n\n# Modified Text Encoder with proper token handling\nclass TextEncoder(nn.Module):\n    def __init__(self, vocab_size=10000, embed_dim=128):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn = nn.GRU(embed_dim, 256, batch_first=True)\n        self.linear = nn.Linear(256, 128)\n        \n        # Build vocabulary from dataset\n        self.word2idx = {}\n        self.idx2word = {}\n        self._build_vocab()\n\n    def _build_vocab(self):\n        all_captions = []\n        for _, caption in dataset:\n            all_captions.extend(caption.split())\n        \n        unique_words = list(set(all_captions))\n        self.word2idx = {word: idx+2 for idx, word in enumerate(unique_words)}  # +2 for padding/unknown\n        self.word2idx['<pad>'] = 0\n        self.word2idx['<unk>'] = 1\n        self.idx2word = {v: k for k, v in self.word2idx.items()}\n\n    def _text_to_indices(self, text):\n        return [self.word2idx.get(word, 1) for word in text.split()]  # 1 for unknown\n\n    def forward(self, text_list):\n        # Convert text to indices\n        indexed = [self._text_to_indices(text) for text in text_list]\n        \n        # Convert to tensor with padding\n        lengths = torch.tensor([len(seq) for seq in indexed])\n        padded = torch.zeros(len(indexed), max(lengths)).long()\n        for i, seq in enumerate(indexed):\n            padded[i, :len(seq)] = torch.tensor(seq)\n        \n        padded = padded.to(device)\n        embedded = self.embedding(padded)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), \n                                                 batch_first=True, enforce_sorted=False)\n        _, hidden = self.rnn(packed)\n        return self.linear(hidden[-1])\n\n\n\n    \n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T02:24:26.006145Z","iopub.execute_input":"2025-05-05T02:24:26.006522Z","iopub.status.idle":"2025-05-05T02:24:26.819397Z","shell.execute_reply.started":"2025-05-05T02:24:26.006493Z","shell.execute_reply":"2025-05-05T02:24:26.818520Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Create image saving directory\nimport torchvision.utils as vutils\nimport matplotlib.pyplot as plt\nos.makedirs('/kaggle/working/generated_images', exist_ok=True)\n\ndef save_generated_images(epoch, netG, text_encoder, device):\n    \"\"\"Save 3x3 grid of generated images\"\"\"\n    netG.eval()\n    text_encoder.eval()\n    \n    # Sample text prompts (modify based on your dataset)\n    sample_texts = [\n        \"a small blue bird\", \"a red bird with black wings\",\n        \"yellow bird sitting on branch\", \"brown and white sparrow\",\n        \"black crow with shiny feathers\", \"white egret in flight\",\n        \"green parrot with red beak\", \"woodpecker with striped head\",\n        \"flamingo standing in water\"\n    ]\n    \n    with torch.no_grad():\n        # Process text embeddings\n        text_embed = text_encoder(sample_texts).to(device)\n        \n        # Generate images\n        noise = torch.randn(9, 100).to(device)  # 9 images for 3x3 grid\n        fake_images = netG(noise, text_embed).cpu()\n    \n    # Denormalize images from [-1,1] to [0,1]\n    fake_images = (fake_images + 1) / 2\n    \n    # Create and save grid\n    grid = vutils.make_grid(fake_images, nrow=3, padding=2)\n    plt.figure(figsize=(8,8))\n    plt.axis(\"off\")\n    plt.title(f\"Generated Images - Epoch {epoch}\")\n    plt.imshow(grid.permute(1, 2, 0))\n    plt.savefig(f'/kaggle/working/generated_images/epoch_{epoch}.png')\n    plt.close()\n    \n    netG.train()\n    text_encoder.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T02:24:46.210293Z","iopub.execute_input":"2025-05-05T02:24:46.210644Z","iopub.status.idle":"2025-05-05T02:24:46.216972Z","shell.execute_reply.started":"2025-05-05T02:24:46.210618Z","shell.execute_reply":"2025-05-05T02:24:46.216255Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Training Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize models\nnetG = Generator().to(device)\nnetD = Discriminator().to(device)\ntext_encoder = TextEncoder().to(device)\n\n# Optimizers\noptimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n# Loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# Training Loop\nnum_epochs = 200\n# Modified training loop with proper graph handling\nfor epoch in range(num_epochs):\n    for i, (real_images, captions) in enumerate(dataloader):\n        batch_size = real_images.size(0)\n        \n        # Prepare data\n        real_images = real_images.to(device)\n        \n        # Get text embeddings (keep gradient for generator)\n        text_embed = text_encoder(captions).to(device)\n        \n        # Train Discriminator\n        netD.zero_grad()\n        \n        # Real images with detached text embeddings\n        output_real = netD(real_images, text_embed.detach())\n        errD_real = criterion(output_real, torch.ones(batch_size).to(device))\n        \n        # Fake images with full graph\n        noise = torch.randn(batch_size, 100).to(device)\n        with torch.no_grad():\n            fake_images = netG(noise, text_embed)\n            \n        # Discriminator forward with detached inputs\n        output_fake = netD(fake_images.detach(), text_embed.detach())\n        errD_fake = criterion(output_fake, torch.zeros(batch_size).to(device))\n        \n        # Discriminator backward\n        errD = errD_real + errD_fake\n        errD.backward()\n        optimizerD.step()\n        \n        # Train Generator\n        netG.zero_grad()\n        \n        # Generate new fake images with gradient\n        fake_images = netG(noise, text_embed)\n        \n        # Discriminator evaluation with text embeddings\n        output = netD(fake_images, text_embed)\n        errG = criterion(output, torch.ones(batch_size).to(device))\n        \n        # Generator backward\n        errG.backward()\n        optimizerG.step()\n\n    if (epoch + 1) % 25 == 0 or epoch == 0:\n        save_generated_images(epoch+1, netG, text_encoder, device)\n        \n    print(f'Epoch [{epoch+1}/{num_epochs}] Loss D: {errD.item():.4f} Loss G: {errG.item():.4f}')\n\n# Save models\ntorch.save(netG.state_dict(), 'generator.pth')\ntorch.save(netD.state_dict(), 'discriminator.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T02:24:53.591789Z","iopub.execute_input":"2025-05-05T02:24:53.592100Z","iopub.status.idle":"2025-05-05T04:56:38.906842Z","shell.execute_reply.started":"2025-05-05T02:24:53.592074Z","shell.execute_reply":"2025-05-05T04:56:38.905676Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/200] Loss D: 0.5645 Loss G: 3.2629\nEpoch [2/200] Loss D: 0.8347 Loss G: 3.2872\nEpoch [3/200] Loss D: 0.7153 Loss G: 5.2462\nEpoch [4/200] Loss D: 0.8308 Loss G: 2.1147\nEpoch [5/200] Loss D: 0.8490 Loss G: 3.5733\nEpoch [6/200] Loss D: 0.5237 Loss G: 2.8441\nEpoch [7/200] Loss D: 0.7282 Loss G: 5.4472\nEpoch [8/200] Loss D: 0.2940 Loss G: 4.1658\nEpoch [9/200] Loss D: 0.6381 Loss G: 3.5027\nEpoch [10/200] Loss D: 0.5702 Loss G: 3.6230\nEpoch [11/200] Loss D: 0.5026 Loss G: 5.1574\nEpoch [12/200] Loss D: 0.9674 Loss G: 1.9625\nEpoch [13/200] Loss D: 1.4796 Loss G: 2.2233\nEpoch [14/200] Loss D: 0.4612 Loss G: 2.1228\nEpoch [15/200] Loss D: 0.3734 Loss G: 4.4133\nEpoch [16/200] Loss D: 0.2362 Loss G: 5.5360\nEpoch [17/200] Loss D: 0.5464 Loss G: 2.9254\nEpoch [18/200] Loss D: 0.2781 Loss G: 3.7782\nEpoch [19/200] Loss D: 0.5230 Loss G: 2.5578\nEpoch [20/200] Loss D: 0.3070 Loss G: 5.3695\nEpoch [21/200] Loss D: 0.2167 Loss G: 5.1027\nEpoch [22/200] Loss D: 0.2797 Loss G: 6.9065\nEpoch [23/200] Loss D: 0.2183 Loss G: 6.3564\nEpoch [24/200] Loss D: 0.1521 Loss G: 5.5538\nEpoch [25/200] Loss D: 0.2680 Loss G: 4.8939\nEpoch [26/200] Loss D: 0.6641 Loss G: 4.3958\nEpoch [27/200] Loss D: 0.3124 Loss G: 3.9335\nEpoch [28/200] Loss D: 0.2287 Loss G: 5.9871\nEpoch [29/200] Loss D: 0.2968 Loss G: 2.8665\nEpoch [30/200] Loss D: 0.4869 Loss G: 9.8263\nEpoch [31/200] Loss D: 0.2454 Loss G: 2.2851\nEpoch [32/200] Loss D: 0.1852 Loss G: 5.1177\nEpoch [33/200] Loss D: 1.8347 Loss G: 5.0757\nEpoch [34/200] Loss D: 0.2285 Loss G: 4.1361\nEpoch [35/200] Loss D: 0.1244 Loss G: 5.5468\nEpoch [36/200] Loss D: 0.1141 Loss G: 4.2851\nEpoch [37/200] Loss D: 0.0932 Loss G: 4.3675\nEpoch [38/200] Loss D: 0.0781 Loss G: 4.2400\nEpoch [39/200] Loss D: 0.1658 Loss G: 9.7462\nEpoch [40/200] Loss D: 0.0818 Loss G: 6.4395\nEpoch [41/200] Loss D: 0.0505 Loss G: 5.1418\nEpoch [42/200] Loss D: 0.1186 Loss G: 6.5667\nEpoch [43/200] Loss D: 0.0708 Loss G: 5.1086\nEpoch [44/200] Loss D: 0.0566 Loss G: 6.0393\nEpoch [45/200] Loss D: 0.1718 Loss G: 3.9203\nEpoch [46/200] Loss D: 0.0982 Loss G: 4.5254\nEpoch [47/200] Loss D: 0.1573 Loss G: 5.5521\nEpoch [48/200] Loss D: 1.5485 Loss G: 1.2876\nEpoch [49/200] Loss D: 0.2270 Loss G: 7.0069\nEpoch [50/200] Loss D: 0.0480 Loss G: 6.9676\nEpoch [51/200] Loss D: 0.0742 Loss G: 5.3431\nEpoch [52/200] Loss D: 0.8087 Loss G: 0.6692\nEpoch [53/200] Loss D: 0.1175 Loss G: 7.3162\nEpoch [54/200] Loss D: 1.2832 Loss G: 0.7884\nEpoch [55/200] Loss D: 0.1046 Loss G: 4.5948\nEpoch [56/200] Loss D: 0.2793 Loss G: 6.3251\nEpoch [57/200] Loss D: 0.0507 Loss G: 6.5188\nEpoch [58/200] Loss D: 0.4642 Loss G: 4.2187\nEpoch [59/200] Loss D: 0.1525 Loss G: 5.7480\nEpoch [60/200] Loss D: 0.1609 Loss G: 5.0579\nEpoch [61/200] Loss D: 0.3342 Loss G: 1.9300\nEpoch [62/200] Loss D: 0.1069 Loss G: 4.9343\nEpoch [63/200] Loss D: 0.6137 Loss G: 9.3400\nEpoch [64/200] Loss D: 0.3236 Loss G: 3.7226\nEpoch [65/200] Loss D: 0.7739 Loss G: 0.4351\nEpoch [66/200] Loss D: 0.4052 Loss G: 7.9552\nEpoch [67/200] Loss D: 0.0747 Loss G: 4.8835\nEpoch [68/200] Loss D: 0.1294 Loss G: 6.0143\nEpoch [69/200] Loss D: 0.2317 Loss G: 7.8110\nEpoch [70/200] Loss D: 0.0820 Loss G: 5.8567\nEpoch [71/200] Loss D: 0.0896 Loss G: 4.2230\nEpoch [72/200] Loss D: 0.1762 Loss G: 4.8565\nEpoch [73/200] Loss D: 0.3251 Loss G: 6.9068\nEpoch [74/200] Loss D: 0.2585 Loss G: 3.7604\nEpoch [75/200] Loss D: 0.4456 Loss G: 2.0575\nEpoch [76/200] Loss D: 0.3063 Loss G: 6.4771\nEpoch [77/200] Loss D: 0.1119 Loss G: 5.4172\nEpoch [78/200] Loss D: 0.0706 Loss G: 5.3349\nEpoch [79/200] Loss D: 0.6517 Loss G: 9.9177\nEpoch [80/200] Loss D: 0.1262 Loss G: 4.7290\nEpoch [81/200] Loss D: 0.3909 Loss G: 2.0538\nEpoch [82/200] Loss D: 0.0809 Loss G: 5.3677\nEpoch [83/200] Loss D: 0.1378 Loss G: 5.2409\nEpoch [84/200] Loss D: 0.2322 Loss G: 3.6811\nEpoch [85/200] Loss D: 0.1815 Loss G: 5.6301\nEpoch [86/200] Loss D: 0.6886 Loss G: 7.3569\nEpoch [87/200] Loss D: 0.9951 Loss G: 11.9112\nEpoch [88/200] Loss D: 0.7092 Loss G: 14.8953\nEpoch [89/200] Loss D: 0.1030 Loss G: 4.9089\nEpoch [90/200] Loss D: 0.2998 Loss G: 2.6396\nEpoch [91/200] Loss D: 0.1969 Loss G: 5.0814\nEpoch [92/200] Loss D: 0.3806 Loss G: 2.3837\nEpoch [93/200] Loss D: 0.7799 Loss G: 1.0085\nEpoch [94/200] Loss D: 0.0969 Loss G: 5.3539\nEpoch [95/200] Loss D: 0.0216 Loss G: 4.9654\nEpoch [96/200] Loss D: 0.2484 Loss G: 3.8544\nEpoch [97/200] Loss D: 0.1673 Loss G: 5.7658\nEpoch [98/200] Loss D: 0.0944 Loss G: 4.9733\nEpoch [99/200] Loss D: 0.1014 Loss G: 4.7183\nEpoch [100/200] Loss D: 0.0819 Loss G: 4.6946\nEpoch [101/200] Loss D: 0.1741 Loss G: 5.6986\nEpoch [102/200] Loss D: 0.0644 Loss G: 5.2842\nEpoch [103/200] Loss D: 1.2475 Loss G: 15.7682\nEpoch [104/200] Loss D: 0.1720 Loss G: 6.1183\nEpoch [105/200] Loss D: 0.1260 Loss G: 5.8028\nEpoch [106/200] Loss D: 0.1541 Loss G: 4.9859\nEpoch [107/200] Loss D: 0.0267 Loss G: 6.2686\nEpoch [108/200] Loss D: 0.0824 Loss G: 5.1771\nEpoch [109/200] Loss D: 0.0407 Loss G: 7.8142\nEpoch [110/200] Loss D: 0.2735 Loss G: 5.3587\nEpoch [111/200] Loss D: 0.1501 Loss G: 5.6540\nEpoch [112/200] Loss D: 0.2676 Loss G: 7.5142\nEpoch [113/200] Loss D: 0.2075 Loss G: 6.1742\nEpoch [114/200] Loss D: 0.2977 Loss G: 3.1565\nEpoch [115/200] Loss D: 0.0960 Loss G: 4.8312\nEpoch [116/200] Loss D: 1.3634 Loss G: 5.5636\nEpoch [117/200] Loss D: 0.1023 Loss G: 6.1975\nEpoch [118/200] Loss D: 0.0282 Loss G: 6.2471\nEpoch [119/200] Loss D: 0.1084 Loss G: 4.3828\nEpoch [120/200] Loss D: 0.2951 Loss G: 3.8701\nEpoch [121/200] Loss D: 0.0944 Loss G: 4.4468\nEpoch [122/200] Loss D: 0.0572 Loss G: 6.3744\nEpoch [123/200] Loss D: 0.4475 Loss G: 7.8848\nEpoch [124/200] Loss D: 0.3928 Loss G: 7.0092\nEpoch [125/200] Loss D: 0.1800 Loss G: 6.4651\nEpoch [126/200] Loss D: 0.0511 Loss G: 5.5960\nEpoch [127/200] Loss D: 0.0306 Loss G: 6.0430\nEpoch [128/200] Loss D: 0.0638 Loss G: 5.8904\nEpoch [129/200] Loss D: 0.1579 Loss G: 3.7539\nEpoch [130/200] Loss D: 0.2162 Loss G: 5.6064\nEpoch [131/200] Loss D: 0.1376 Loss G: 4.6032\nEpoch [132/200] Loss D: 0.0366 Loss G: 5.7772\nEpoch [133/200] Loss D: 0.3121 Loss G: 5.3921\nEpoch [134/200] Loss D: 0.1532 Loss G: 4.1754\nEpoch [135/200] Loss D: 0.0335 Loss G: 6.2307\nEpoch [136/200] Loss D: 0.1117 Loss G: 3.8751\nEpoch [137/200] Loss D: 0.2502 Loss G: 5.1535\nEpoch [138/200] Loss D: 0.0568 Loss G: 5.9160\nEpoch [139/200] Loss D: 0.1904 Loss G: 3.6670\nEpoch [140/200] Loss D: 0.1165 Loss G: 4.2549\nEpoch [141/200] Loss D: 0.0321 Loss G: 6.0616\nEpoch [142/200] Loss D: 0.1467 Loss G: 5.9165\nEpoch [143/200] Loss D: 0.0296 Loss G: 6.0402\nEpoch [144/200] Loss D: 0.0730 Loss G: 4.9041\nEpoch [145/200] Loss D: 0.0438 Loss G: 4.9918\nEpoch [146/200] Loss D: 0.5001 Loss G: 9.0839\nEpoch [147/200] Loss D: 0.0708 Loss G: 4.8346\nEpoch [148/200] Loss D: 0.0207 Loss G: 6.0084\nEpoch [149/200] Loss D: 0.1328 Loss G: 5.3048\nEpoch [150/200] Loss D: 0.6922 Loss G: 13.0791\nEpoch [151/200] Loss D: 0.0267 Loss G: 5.2455\nEpoch [152/200] Loss D: 0.1425 Loss G: 4.8675\nEpoch [153/200] Loss D: 3.1234 Loss G: 0.4828\nEpoch [154/200] Loss D: 0.2254 Loss G: 8.1741\nEpoch [155/200] Loss D: 0.2857 Loss G: 3.1192\nEpoch [156/200] Loss D: 0.2360 Loss G: 3.7895\nEpoch [157/200] Loss D: 0.0437 Loss G: 7.0092\nEpoch [158/200] Loss D: 0.0830 Loss G: 4.9588\nEpoch [159/200] Loss D: 0.0497 Loss G: 4.6726\nEpoch [160/200] Loss D: 0.0294 Loss G: 7.1762\nEpoch [161/200] Loss D: 0.0600 Loss G: 5.0977\nEpoch [162/200] Loss D: 0.0816 Loss G: 6.2965\nEpoch [163/200] Loss D: 0.1980 Loss G: 4.4884\nEpoch [164/200] Loss D: 0.2788 Loss G: 1.3385\nEpoch [165/200] Loss D: 0.0761 Loss G: 5.1611\nEpoch [166/200] Loss D: 0.0782 Loss G: 4.9004\nEpoch [167/200] Loss D: 0.1593 Loss G: 5.1153\nEpoch [168/200] Loss D: 0.1009 Loss G: 4.0927\nEpoch [169/200] Loss D: 0.0316 Loss G: 6.3572\nEpoch [170/200] Loss D: 0.0517 Loss G: 5.0707\nEpoch [171/200] Loss D: 0.1849 Loss G: 5.8033\nEpoch [172/200] Loss D: 0.0372 Loss G: 5.3219\nEpoch [173/200] Loss D: 0.1059 Loss G: 4.2278\nEpoch [174/200] Loss D: 0.0882 Loss G: 4.7226\nEpoch [175/200] Loss D: 0.0522 Loss G: 6.3914\nEpoch [176/200] Loss D: 0.0708 Loss G: 7.0121\nEpoch [177/200] Loss D: 6.0707 Loss G: 14.4853\nEpoch [178/200] Loss D: 0.1446 Loss G: 5.3323\nEpoch [179/200] Loss D: 0.1588 Loss G: 4.7030\nEpoch [180/200] Loss D: 0.1011 Loss G: 7.3095\nEpoch [181/200] Loss D: 0.0657 Loss G: 5.6744\nEpoch [182/200] Loss D: 0.1756 Loss G: 5.9023\nEpoch [183/200] Loss D: 0.0168 Loss G: 6.9617\nEpoch [184/200] Loss D: 0.1084 Loss G: 4.2443\nEpoch [185/200] Loss D: 0.0581 Loss G: 4.9087\nEpoch [186/200] Loss D: 0.6413 Loss G: 10.2175\nEpoch [187/200] Loss D: 0.0723 Loss G: 6.2132\nEpoch [188/200] Loss D: 0.6123 Loss G: 0.7244\nEpoch [189/200] Loss D: 0.0563 Loss G: 5.0037\nEpoch [190/200] Loss D: 0.1914 Loss G: 7.5462\nEpoch [191/200] Loss D: 0.0373 Loss G: 5.8842\nEpoch [192/200] Loss D: 0.0124 Loss G: 6.4013\nEpoch [193/200] Loss D: 0.0115 Loss G: 6.9330\nEpoch [194/200] Loss D: 0.0903 Loss G: 6.3101\nEpoch [195/200] Loss D: 0.2545 Loss G: 2.9797\nEpoch [196/200] Loss D: 0.1054 Loss G: 5.9340\nEpoch [197/200] Loss D: 0.1006 Loss G: 5.8231\nEpoch [198/200] Loss D: 0.2476 Loss G: 7.7654\nEpoch [199/200] Loss D: 0.0515 Loss G: 6.3031\nEpoch [200/200] Loss D: 0.0718 Loss G: 6.9210\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Compress directory\n!zip -r generated_images.zip /kaggle/working/generated_images\n\n# Create download link\nFileLink('generated_images.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:56:55.648442Z","iopub.execute_input":"2025-05-05T04:56:55.648795Z","iopub.status.idle":"2025-05-05T04:56:55.875488Z","shell.execute_reply.started":"2025-05-05T04:56:55.648764Z","shell.execute_reply":"2025-05-05T04:56:55.874659Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/generated_images/ (stored 0%)\n  adding: kaggle/working/generated_images/epoch_125.png (deflated 5%)\n  adding: kaggle/working/generated_images/epoch_25.png (deflated 5%)\n  adding: kaggle/working/generated_images/epoch_75.png (deflated 5%)\n  adding: kaggle/working/generated_images/epoch_200.png (deflated 5%)\n  adding: kaggle/working/generated_images/epoch_1.png (deflated 4%)\n  adding: kaggle/working/generated_images/epoch_50.png (deflated 5%)\n  adding: kaggle/working/generated_images/epoch_100.png (deflated 5%)\n  adding: kaggle/working/generated_images/epoch_175.png (deflated 5%)\n  adding: kaggle/working/generated_images/epoch_150.png (deflated 5%)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/generated_images.zip","text/html":"<a href='generated_images.zip' target='_blank'>generated_images.zip</a><br>"},"metadata":{}}],"execution_count":7}]}